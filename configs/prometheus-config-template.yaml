apiVersion: v1
kind: Template
metadata:
  name: prometheus-config-template

parameters:
- name: CLUSTER_NAME
  required: true
- name: ROUTER_BASIC_USERNAME
  value: admin
  required: true
- name: ROUTER_BASIC_PASSWORD
  required: true

objects:
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: prom-k8s
  data:
    docker.rules: |-
      ALERT DockerHasErrors
        IF irate(kubelet_docker_operations_errors{operation_type!~"remove_container|start_container|inspect_container|inspect_image|inspect_exec|operation_type||logs"}[10m])  > 0
        FOR 5m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "docker",
          severity = "critical",
        }
        ANNOTATIONS {
          summary = "Docker has errors",
          description = "Docker on instance {{ $labels.instance }} has errors with {{ $labels.operation_type}}",
        }

      ALERT DockerHasTimeout
        IF irate(kubelet_docker_operations_timeout[5m]) > 0
        FOR 1m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "docker",
          severity = "critical",
        }
        ANNOTATIONS {
          summary = "Docker Timeout",
          description = "Docker on {{ $labels.instance }} has timeouts",
        }

    kubernetes.rules: |
      # NOTE: These rules were kindly contributed by the SoundCloud engineering team.
      ### Container resources ###
      cluster_namespace_controller_pod_container:spec_memory_limit_bytes =
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_spec_memory_limit_bytes{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )

      cluster_namespace_controller_pod_container:spec_cpu_shares =
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_spec_cpu_shares{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )

      cluster_namespace_controller_pod_container:cpu_usage:rate =
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            irate(
              container_cpu_usage_seconds_total{container_name!=""}[5m]
            ),
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )

      cluster_namespace_controller_pod_container:memory_usage:bytes =
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_memory_usage_bytes{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )

      cluster_namespace_controller_pod_container:memory_working_set:bytes =
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_memory_working_set_bytes{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )

      cluster_namespace_controller_pod_container:memory_rss:bytes =
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_memory_rss{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )

      cluster_namespace_controller_pod_container:memory_cache:bytes =
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_memory_cache{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )

      cluster_namespace_controller_pod_container:disk_usage:bytes =
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_disk_usage_bytes{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )

      cluster_namespace_controller_pod_container:memory_pagefaults:rate =
        sum by (cluster,namespace,controller,pod_name,container_name,scope,type) (
          label_replace(
            irate(
              container_memory_failures_total{container_name!=""}[5m]
            ),
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )

      cluster_namespace_controller_pod_container:memory_oom:rate =
        sum by (cluster,namespace,controller,pod_name,container_name,scope,type) (
          label_replace(
            irate(
              container_memory_failcnt{container_name!=""}[5m]
            ),
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )

      ### Cluster resources ###
      cluster:memory_allocation:percent =
        100 * sum by (cluster) (
          container_spec_memory_limit_bytes{pod_name!=""}
        ) / sum by (cluster) (
          machine_memory_bytes
        )

      cluster:memory_used:percent =
        100 * sum by (cluster) (
          container_memory_usage_bytes{pod_name!=""}
        ) / sum by (cluster) (
          machine_memory_bytes
        )

      cluster:cpu_allocation:percent =
        100 * sum by (cluster) (
          container_spec_cpu_shares{pod_name!=""}
        ) / sum by (cluster) (
          container_spec_cpu_shares{id="/"} * on(cluster,instance) machine_cpu_cores
        )

      cluster:node_cpu_use:percent =
        100 * sum by (cluster) (
          rate(node_cpu{mode!="idle"}[5m])
        ) / sum by (cluster) (
          machine_cpu_cores
        )

      ### API latency ###
      # Raw metrics are in microseconds. Convert to seconds.
      cluster_resource_verb:apiserver_latency:quantile_seconds{quantile="0.99"} =
        histogram_quantile(
          0.99,
          sum by(le,cluster,job,resource,verb) (apiserver_request_latencies_bucket)
        ) / 1e6
      cluster_resource_verb:apiserver_latency:quantile_seconds{quantile="0.9"} =
        histogram_quantile(
          0.9,
          sum by(le,cluster,job,resource,verb) (apiserver_request_latencies_bucket)
        ) / 1e6
      cluster_resource_verb:apiserver_latency:quantile_seconds{quantile="0.5"} =
        histogram_quantile(
          0.5,
          sum by(le,cluster,job,resource,verb) (apiserver_request_latencies_bucket)
        ) / 1e6

      ### Scheduling latency ###
      cluster:scheduler_e2e_scheduling_latency:quantile_seconds{quantile="0.99"} =
        histogram_quantile(0.99,sum by (le,cluster) (scheduler_e2e_scheduling_latency_microseconds_bucket)) / 1e6
      cluster:scheduler_e2e_scheduling_latency:quantile_seconds{quantile="0.9"} =
        histogram_quantile(0.9,sum by (le,cluster) (scheduler_e2e_scheduling_latency_microseconds_bucket)) / 1e6
      cluster:scheduler_e2e_scheduling_latency:quantile_seconds{quantile="0.5"} =
        histogram_quantile(0.5,sum by (le,cluster) (scheduler_e2e_scheduling_latency_microseconds_bucket)) / 1e6

      cluster:scheduler_scheduling_algorithm_latency:quantile_seconds{quantile="0.99"} =
        histogram_quantile(0.99,sum by (le,cluster) (scheduler_scheduling_algorithm_latency_microseconds_bucket)) / 1e6
      cluster:scheduler_scheduling_algorithm_latency:quantile_seconds{quantile="0.9"} =
        histogram_quantile(0.9,sum by (le,cluster) (scheduler_scheduling_algorithm_latency_microseconds_bucket)) / 1e6
      cluster:scheduler_scheduling_algorithm_latency:quantile_seconds{quantile="0.5"} =
        histogram_quantile(0.5,sum by (le,cluster) (scheduler_scheduling_algorithm_latency_microseconds_bucket)) / 1e6

      cluster:scheduler_binding_latency:quantile_seconds{quantile="0.99"} =
        histogram_quantile(0.99,sum by (le,cluster) (scheduler_binding_latency_microseconds_bucket)) / 1e6
      cluster:scheduler_binding_latency:quantile_seconds{quantile="0.9"} =
        histogram_quantile(0.9,sum by (le,cluster) (scheduler_binding_latency_microseconds_bucket)) / 1e6
      cluster:scheduler_binding_latency:quantile_seconds{quantile="0.5"} =
        histogram_quantile(0.5,sum by (le,cluster) (scheduler_binding_latency_microseconds_bucket)) / 1e6

      ALERT K8SFailedSchedulingErrorTooHigh
        IF sum(rate(heptio_eventrouter_warnings_total{reason="FailedScheduling"}[5m])) > 0.1
        FOR 5m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "warning"
        }
        ANNOTATIONS {
          summary = "Too many scheduling failures",
          description = "Kubernetes is failing to schedule pods. Please check nodes readiness",
        }

      ALERT K8SNodeDown
        IF up{job="kubernetes-nodes"} == 0
        FOR 1h
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "warning"
        }
        ANNOTATIONS {
          summary = "Kubelet cannot be scraped",
          description = "Prometheus could not scrape a {{ $labels.job }} for more than one hour",
        }

      ALERT K8SNodeNotReady
        IF sum(kube_node_status_condition{condition="Ready", status=~"(false|unknown)"}) by (instance) > 0
        FOR 1m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "warning",
        }
        ANNOTATIONS {
          summary = "Node status is NotReady",
          description = "The Kubelet on {{ $labels.node }} has not checked in with the API, or has set itself to NotReady, for more than 2 minutes",
        }

      ALERT K8SKubeletNodeExporterDown
        IF up{job="node-exporter"} == 0
        FOR 15m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "warning"
        }
        ANNOTATIONS {
          summary = "Kubelet node_exporter cannot be scraped",
          description = "Prometheus could not scrape a {{ $labels.job }} for more than one hour.",
        }

      ALERT K8SKubeletDown
        IF absent(up{job="kubernetes-nodes"}) or count by (cluster) (up{job="kubernetes-nodes"} == 0) / count by (cluster) (up{job="kubernetes-nodes"}) > 0.1
        FOR 1h
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "critical"
        }
        ANNOTATIONS {
          summary = "Many Kubelets cannot be scraped",
          description = "Prometheus failed to scrape more than 10% of kubelets, or all Kubelets have disappeared from service discovery.",
        }

      ALERT K8SApiserverDown
        IF up{job="kubernetes-apiservers"} == 0
        FOR 15m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "warning"
        }
        ANNOTATIONS {
          summary = "API server unreachable",
          description = "An API server could not be scraped.",
        }

      # Disable for non HA kubernetes setups.
      ALERT K8SApiserverDown
        IF absent({job="kubernetes-apiservers"}) or (count by(cluster) (up{job="kubernetes-apiservers"} == 1) < count by(cluster) (up{job="kubernetes-apiservers"}))
        FOR 5m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "critical"
        }
        ANNOTATIONS {
          summary = "API server unreachable",
          description = "Prometheus failed to scrape multiple API servers, or all API servers have disappeared from service discovery.",
        }

      ALERT K8SConntrackTableFull
        IF 100*node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 50
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "warning"
        }
        ANNOTATIONS {
          summary = "Number of tracked connections is near the limit",
          description = "The nf_conntrack table is {{ $value }}% full.",
        }

      ALERT K8SConntrackTableFull
        IF 100*node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 90
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "critical"
        }
        ANNOTATIONS {
          summary = "Number of tracked connections is near the limit",
          description = "The nf_conntrack table is {{ $value }}% full.",
        }

      # To catch the conntrack sysctl de-tuning when it happens
      ALERT K8SConntrackTuningMissing
        IF node_nf_conntrack_udp_timeout > 10
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "warning",
        }
        ANNOTATIONS {
          summary = "Node does not have the correct conntrack tunings",
          description = "Nodes keep un-setting the correct tunings, investigate when it happens.",
        }

      ALERT K8STooManyOpenFiles
        IF 100*process_open_fds{job=~"kubelet|kubernetes"} / process_max_fds > 50
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "warning"
        }
        ANNOTATIONS {
          summary = "{{ $labels.job }} has too many open file descriptors",
          description = "{{ $labels.node }} is using {{ $value }}% of the available file/socket descriptors.",
        }

      ALERT K8STooManyOpenFiles
        IF 100*process_open_fds{job=~"kubelet|kubernetes"} / process_max_fds > 80
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "critical"
        }
        ANNOTATIONS {
          summary = "{{ $labels.job }} has too many open file descriptors",
          description = "{{ $labels.node }} is using {{ $value }}% of the available file/socket descriptors.",
        }

      # Some verbs excluded because they are expected to be long-lasting:
      # WATCHLIST is long-poll, CONNECT is `kubectl exec`.
      ALERT K8SApiServerLatency
        IF histogram_quantile(
            0.99,
            sum without (instance,node,resource) (apiserver_request_latencies_bucket{verb=~"POST|GET|DELETE|PATCH"})
          ) / 1e6 > 2.0
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "warning"
        }
        ANNOTATIONS {
          summary = "Kubernetes apiserver latency is high",
          description = "99th percentile Latency for {{ $labels.verb }} requests to the kube-apiserver is higher than 2s.",
        }

      ALERT K8SApiServerEtcdAccessLatency
        IF etcd_request_latencies_summary{quantile="0.99"} / 1e6 > 1.0
        FOR 15m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "warning"
        }
        ANNOTATIONS {
          summary = "Access to etcd is slow",
          description = "99th percentile latency for apiserver to access etcd is higher than 1s.",
        }

      ALERT K8SKubeletTooManyPods
        IF sum(label_replace(kubelet_running_pod_count, "node", "$1", "instance", "(.*)") * on(node) group_right() kube_node_labels) by(node) > sum(kube_node_status_allocatable_pods) by(node)
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "k8s",
          severity = "warning",
        }
        ANNOTATIONS {
          summary = "Kubelet is close to pod limit",
          description = "Kubelet {{$labels.instance}} is running {{$value}} pods",
        }

    node.rules: |-
      ALERT MasterCPUUsage
        IF ((1 - (avg(irate(node_cpu{mode="idle", role="master"}[5m])) BY (instance))) * 100) > 85
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "infra",
          severity="critical"
        }
        ANNOTATIONS {
          summary = "High CPU usage detected",
          description = "{{$labels.instance}} ({{$labels.label_type}}): CPU usage is above 85% (current value is: {{ $value }})"
        }
      ALERT MasterLoadAverage
        IF node_load15{role="master"} / ON(server_name) group_right() machine_cpu_cores{role="master"} > 2
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "infra",
          severity="critical"
        }
        ANNOTATIONS {
          summary = "High load average detected",
          description = "{{$labels.instance}} ({{$labels.label_type}}): load average (15) is high"
        }
      ALERT MasterMemoryUsage
        IF ((node_memory_MemTotal{role="master"} - node_memory_MemAvailable{role="master"}) / (node_memory_MemTotal{role="master"}) * 100) > 85
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "infra",
          severity="critical"
        }
        ANNOTATIONS {
          summary = "High memory usage detected",
          description = "{{$labels.instance}} ({{$labels.label_type}}): Memory usage is above 85% (current value is: {{ $value }})"
        }

      ALERT InfraCPUUsage
        IF ((1 - (avg(irate(node_cpu{mode="idle", role="infra"}[5m])) BY (instance))) * 100) > 85
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "infra",
          severity="critical"
        }
        ANNOTATIONS {
          summary = "High CPU usage detected",
          description = "{{$labels.instance}} ({{$labels.label_type}}): CPU usage is above 85% (current value is: {{ $value }})"
        }
      ALERT InfraLoadAverage
        IF node_load15{role="infra"} / ON(server_name) group_right() machine_cpu_cores{role="infra"} > 2
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "infra",
          severity="critical"
        }
        ANNOTATIONS {
          summary = "High load average detected",
          description = "{{$labels.instance}} ({{$labels.label_type}}): load average (15) is high"
        }
      ALERT InfraMemoryUsage
        IF ((node_memory_MemTotal{role="infra"} - node_memory_MemAvailable{role="infra"}) / (node_memory_MemTotal{role="infra"}) * 100) > 85
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "infra",
          severity="critical"
        }
        ANNOTATIONS {
          summary = "High memory usage detected",
          description = "{{$labels.instance}} ({{$labels.label_type}}): Memory usage is above 85% (current value is: {{ $value }})"
        }

      ALERT NodeCPUUsage
        IF ((1 - (avg(irate(node_cpu{mode="idle", role="app"}[5m])) BY (instance))) * 100) > 85
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "infra",
          severity="critical"
        }
        ANNOTATIONS {
          summary = "High CPU usage detected",
          description = "{{$labels.instance}} ({{$labels.label_type}}): CPU usage is above 85% (current value is: {{ $value }})"
        }
      ALERT NodeLoadAverage
        IF node_load15{role="app"} / ON(server_name) group_right() machine_cpu_cores{role="app"} > 2
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "infra",
          severity="critical"
        }
        ANNOTATIONS {
          summary = "High load average detected",
          description = "{{$labels.instance}} ({{$labels.label_type}}): load average (15) is high"
        }
      ALERT NodeMemoryUsage
        IF ((node_memory_MemTotal{role="app"} - node_memory_MemAvailable{role="app"}) / (node_memory_MemTotal{role="app"}) * 100) > 85
        FOR 10m
        LABELS {
          cluster = "${CLUSTER_NAME}",
          service = "infra",
          severity="critical"
        }
        ANNOTATIONS {
          summary = "High memory usage detected",
          description = "{{$labels.instance}} ({{$labels.label_type}}): Memory usage is above 85% (current value is: {{ $value }})"
        }

      #ALERT NodeLowRootDisk
      #  IF (label_replace(max(((node_filesystem_size{fstype="rootfs"} - node_filesystem_free{fstype="rootfs"} ) / node_filesystem_size{fstype="rootfs"} * 100)) by(instance), "node", "$1", "instance", "(.*)") * on(node) group_right() kube_node_labels) > 80
      #  FOR 10m
      #  LABELS {
      #    severity="critical"
      #  }
      #  ANNOTATIONS {
      #    summary = "Low root disk space",
      #    description = "{{$labels.instance}} ({{$labels.label_type}}): Root disk usage is above 80% (current value is: {{ $value }})"
      #  }
      #
      ## Docker Disk (80% full)
      #ALERT NodeLowDockerDisk
      #  IF (label_replace(max(((node_filesystem_size{device="rootfs"} - node_filesystem_free{device="rootfs"} ) / node_filesystem_size{device="rootfs"} * 100)) by(instance), "node", "$1", "instance", "(.*)") * on(node) group_right() kube_node_labels) > 80
      #  FOR 10m
      #  LABELS {
      #    severity="critical"
      #  }
      #  ANNOTATIONS {
      #    summary = "Low data disk space",
      #    description = "{{$labels.instance}} ({{$labels.label_type}}): Data disk usage is above 80% (current value is: {{ $value }})"
      #  }
      #
      ## OpenShift Disk (80% full)
      #ALERT NodeLowDataDisk
      #  IF (label_replace(max(((node_filesystem_size{device="/dev/xvdc"} - node_filesystem_free{device="/dev/xvdc"} ) / node_filesystem_size{device="/dev/xvdc"} * 100)) by(instance), "node", "$1", "instance", "(.*)") * on(node) group_right() kube_node_labels) > 80
      #  FOR 10m
      #  LABELS {
      #    severity="critical"
      #  }
      #  ANNOTATIONS {
      #    summary = "Low data disk space",
      #    description = "{{$labels.instance}} ({{$labels.label_type}}): Data disk usage is above 80% (current value is: {{ $value }})"
      #  }


    prometheus.yml: |-
      global:
        scrape_interval: 30s
        scrape_timeout: 30s

      rule_files:
      - /etc/prometheus/*.rules

      scrape_configs:
        - job_name: prometheus
          static_configs:
            - targets: ['localhost:9090']
          relabel_configs:
          - source_labels: [__address__]
            action: replace
            regex: .*
            target_label: role
            replacement: prom

        - job_name: aws
          static_configs:
            - targets: ['cloudwatch-exporter:9106']
          relabel_configs:
          - source_labels: [__address__]
            action: replace
            regex: .*
            target_label: role
            replacement: ec2

        #- job_name: etcd
        #  scheme: https
        #  tls_config:
        #    insecure_skip_verify: true
        #  static_configs:
        #    - targets:
        #      - 'ip-10-0-1-129.ec2.internal:2379'
        #      - 'ip-10-0-2-63.ec2.internal:2379'
        #      - 'ip-10-0-3-166.ec2.internal:2379'

        - job_name: kubernetes-nodes
          scheme: https
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
          - role: node

          relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics

        - job_name: kubernetes-apiservers
          scheme: https
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
          - role: endpoints

          relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

        - job_name: kubernetes-services
          kubernetes_sd_configs:
          - role: endpoints

          relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: kubernetes_name
          ## drop this taget since we cant scrap basic user/pass into labels
          ## https://github.com/prometheus/prometheus/issues/2614
          - source_labels: [__meta_kubernetes_service_name]
            action: drop
            regex: router

        - job_name: "haproxy"
          basic_auth:
            username: ${ROUTER_BASIC_USERNAME}
            password: ${ROUTER_BASIC_PASSWORD}

          dns_sd_configs:
            - names:
              - haproxy.infra.getupcloud.com
              type: SRV
          relabel_configs:
          - source_labels: [__address__]
            action: replace
            regex: .*
            target_label: role
            replacement: infra

        - job_name: "master-nodes"
          dns_sd_configs:
            - names:
              - m.infra.getupcloud.com
              type: SRV
          relabel_configs:
          - source_labels: [__address__]
            action: replace
            target_label: server_name
            regex: ([^\.]+).*
          - source_labels: [__address__]
            action: replace
            regex: .*
            target_label: role
            replacement: infra

        - job_name: "infra-nodes"
          dns_sd_configs:
            - names:
              - i.infra.getupcloud.com
              type: SRV
          relabel_configs:
          - source_labels: [__address__]
            action: replace
            target_label: server_name
            regex: ([^\.]+).*
          - source_labels: [__address__]
            action: replace
            regex: .*
            target_label: role
            replacement: infra

        - job_name: "app-nodes"
          dns_sd_configs:
            - names:
              - a.infra.getupcloud.com
              type: SRV
          relabel_configs:
          - source_labels: [__address__]
            action: replace
            target_label: server_name
            regex: ([^\.]+).*
          - source_labels: [__address__]
            action: replace
            regex: .*
            target_label: role
            replacement: app

